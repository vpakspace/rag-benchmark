A Survey of Large Language Models: Architecture, Training, and Emergent Capabilities

Based on: Zhao, W. X., Zhou, K., Li, J., Tang, J., Wang, X., Hou, Y., ... & Wen, J. R. (2023).
A survey of large language models. arXiv:2303.18223.

---

INTRODUCTION

The field of natural language processing has undergone a profound transformation over the past decade, driven by successive waves of architectural and methodological innovation. Language modeling—the task of estimating the probability distribution over sequences of words—has served as a central organizing objective throughout this evolution. What began as an engineering challenge of building statistical n-gram models has become one of the defining research frontiers of modern artificial intelligence, culminating in systems capable of open-domain dialogue, complex reasoning, code synthesis, and creative writing.

Large language models (LLMs) are neural network systems trained on massive text corpora to predict or generate natural language. The defining characteristic that separates LLMs from their predecessors is scale: both the number of model parameters (ranging from billions to trillions) and the volume of training data (measured in hundreds of billions to trillions of tokens) are orders of magnitude larger than what characterized earlier models. This scaling produces systems with qualitatively different capabilities, including the ability to perform tasks described only in natural language instructions without any task-specific fine-tuning—a property known as in-context learning or few-shot prompting.

The history leading to modern LLMs can be divided into several overlapping phases. During the 1990s and 2000s, statistical language models based on n-gram frequency counts dominated the field. These models were interpretable and efficient but fundamentally limited by the curse of dimensionality: they could not generalize well across semantically related but syntactically distinct expressions, and their performance plateaued as corpus size grew. The introduction of neural network language models in the early 2010s, particularly through the work of Bengio et al. (2003) on distributed word representations and Mikolov et al.'s Word2Vec (2013), established that learned dense vector representations could capture semantic relationships that n-grams could not. Recurrent neural networks and their gated variants—Long Short-Term Memory (LSTM, Hochreiter and Schmidhuber, 1997) and Gated Recurrent Units (GRU, Cho et al., 2014)—then enabled sequence-to-sequence learning, making neural machine translation and text summarization tractable.

The watershed moment for the entire field came in June 2017 with the publication of "Attention Is All You Need" by Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin at Google Brain and Google Research. The Transformer architecture they introduced abandoned recurrence entirely in favor of a self-attention mechanism that allows every token in a sequence to directly attend to every other token. This design enabled far more effective parallelization during training, since computations for different positions in the sequence no longer depended sequentially on one another, and it resolved the vanishing gradient problems that plagued RNN-based sequence modeling. The Transformer's encoder-decoder structure, combined with multi-head attention and position-wise feed-forward layers, established the foundational blueprint that all major LLMs have built upon since.


THE GPT SERIES: GENERATIVE PRE-TRAINING AT SCALE

OpenAI introduced the first model in the Generative Pre-trained Transformer series—GPT-1—in June 2018, in a paper by Radford, Narasimhan, Salimans, and Sutskever titled "Improving Language Understanding by Generative Pre-Training." GPT-1 contained 117 million parameters and was trained on the BooksCorpus dataset, which comprised approximately 7,000 unpublished books totaling around 800 million words. The model used only the decoder portion of the Transformer, training it to predict the next token in a sequence using a causal (left-to-right) language modeling objective. The central insight of the GPT-1 paper was that a single large model pre-trained on a diverse text corpus could be fine-tuned with relatively small amounts of labeled data to achieve strong performance across multiple NLP tasks, including textual entailment, question answering, semantic similarity, and document classification. This generative pre-training paradigm proved remarkably effective and established the template for all subsequent GPT models.

OpenAI published GPT-2 in February 2019, authored by Radford, Wu, Child, Luan, Amodei, and Sutskever. GPT-2 scaled the original architecture substantially, reaching 1.5 billion parameters—roughly thirteen times the size of GPT-1—and was trained on WebText, a dataset constructed by scraping outbound links from Reddit posts that received at least three upvotes. WebText contained approximately 40 gigabytes of text across eight million documents. The GPT-2 paper demonstrated that the model could generate coherent multi-paragraph text of high enough quality to be mistaken for human-authored content, and that it achieved state-of-the-art performance on several language modeling benchmarks in a zero-shot setting (i.e., without any task-specific fine-tuning). Notably, OpenAI initially declined to release the full 1.5B parameter model, citing concerns that it could be misused for generating disinformation, fabricated news articles, and spam at scale—a decision that sparked widespread debate about responsible AI development and the ethics of capability disclosure. A staged release followed over subsequent months.

The decisive scaling experiment came with GPT-3, described in the paper "Language Models are Few-Shot Learners" by Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei, published in May 2020. GPT-3 comprised 175 billion parameters, making it more than 100 times larger than GPT-2. It was trained on a mix of datasets including a filtered version of Common Crawl (approximately 410 billion tokens), WebText2 (19 billion tokens), Books1 and Books2 (67 billion tokens combined), and English Wikipedia (3 billion tokens). The total training dataset exceeded 499 billion tokens before filtering and deduplication.

The central finding of the GPT-3 paper was that the model exhibited remarkably capable few-shot and zero-shot learning: given just a handful of input-output examples embedded directly in the prompt (in-context learning), GPT-3 could perform competitive or state-of-the-art on tasks including translation, question answering, cloze tasks, and arithmetic—all without any gradient updates. This demonstrated that scale alone could endow a language model with a form of meta-learning, enabling it to rapidly adapt to new task descriptions from the context window. GPT-3 also showed striking emergent behaviors: it could write functional code, compose coherent essays, and exhibit rudimentary chain-of-thought reasoning when prompted appropriately. The Brown et al. 2020 paper became one of the most cited works in NLP and effectively defined the modern paradigm of prompt-based interaction with LLMs.

InstructGPT, described by Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, Schulman, Hilton, Kelton, Miller, Simens, Askell, Welinder, Christiano, Leike, and Lowe in a paper published in March 2022, addressed a fundamental limitation of models like GPT-3: despite their impressive capabilities, these models were optimized to predict the next token rather than to be helpful, harmless, and honest. The InstructGPT work introduced a three-stage pipeline built on top of a GPT-3 base model. In the first stage, human annotators wrote example demonstrations of desired model behavior and the model was fine-tuned on these examples using supervised learning. In the second stage, annotators ranked multiple model outputs for the same prompt by quality, and these preference data were used to train a reward model that learned to predict human preferences. In the third stage, the language model was further fine-tuned using reinforcement learning with human feedback (RLHF), specifically using the Proximal Policy Optimization (PPO) algorithm developed by Schulman et al. (2017), with the reward signal provided by the trained reward model. This process aligned the model's behavior more closely with human intent and instruction-following. InstructGPT models were preferred by human raters over the raw GPT-3 outputs even when the InstructGPT models had an order of magnitude fewer parameters (1.3B vs. 175B), demonstrating that alignment quality could compensate substantially for scale.

ChatGPT, released publicly on November 30, 2022, applied the RLHF alignment methodology from InstructGPT to a model fine-tuned specifically for multi-turn dialogue. Unlike GPT-3 and InstructGPT, which operated primarily in a single-turn prompt-completion framework, ChatGPT was optimized to maintain coherent context across extended conversations, to refuse inappropriate requests, and to acknowledge uncertainty. Its release marked a pivotal moment in the public perception of AI: within five days it had one million users, and within two months it had reached 100 million monthly active users—the fastest consumer product growth rate ever recorded to that point. ChatGPT demonstrated to a mass audience that LLMs could serve as practical assistants for writing, debugging, analysis, and explanation, fundamentally shifting expectations about the near-term impact of AI.

GPT-4 was released by OpenAI in March 2023, with a corresponding technical report that was notable for its unusual level of opacity: the parameter count, training data composition, and specific architectural details were not disclosed. What the technical report confirmed was that GPT-4 is a large multimodal model capable of accepting both text and image inputs—though at release only text input was available through the API—and producing text outputs. GPT-4 substantially outperformed GPT-3.5 (the model underlying ChatGPT) on academic and professional benchmarks: it scored in approximately the 90th percentile on the simulated Uniform Bar Examination, near the top on the USABO Semifinal Biology Olympiad, and achieved a score of 1410 on the SAT. On the MMLU benchmark across 57 subjects, GPT-4 reached 86.4% accuracy versus 70.0% for GPT-3.5 and 63.4% for the best available models as of late 2022. The GPT-4 technical report also introduced systematic red-teaming methodology, documenting the kinds of harmful capabilities that were identified and mitigated before public release.


BERT AND BIDIRECTIONAL ENCODER MODELS

While the GPT series pursued a unidirectional, decoder-only architecture trained with a causal language modeling objective, Google Research explored a complementary direction: bidirectional encoding. BERT—Bidirectional Encoder Representations from Transformers—was introduced by Devlin, Chang, Lee, and Toutanova in a paper published in October 2018. BERT used only the encoder portion of the Transformer architecture and was trained with two novel pre-training objectives.

The first objective, Masked Language Modeling (MLM), randomly masked 15% of input tokens and trained the model to predict the original masked tokens from the surrounding context. Unlike causal language modeling, which can only attend to previous tokens, MLM allows the model to use both left and right context simultaneously, making it genuinely bidirectional. The second objective, Next Sentence Prediction (NSP), trained the model to predict whether two sentences appeared consecutively in the original corpus or were randomly paired. This objective was intended to capture discourse-level relationships relevant to tasks like question answering and natural language inference.

BERT was released in two configurations: BERT-Base with 110 million parameters (12 Transformer layers, 768-dimensional hidden states, 12 attention heads) and BERT-Large with 340 million parameters (24 layers, 1024-dimensional hidden states, 16 attention heads). Both models were pre-trained on the concatenation of English Wikipedia (2.5 billion words) and the BooksCorpus (800 million words). BERT-Large established new state-of-the-art results on 11 NLP benchmarks simultaneously at the time of publication, including the Stanford Question Answering Dataset (SQuAD), the General Language Understanding Evaluation (GLUE) benchmark, and the Situations With Adversarial Generations (SWAG) dataset. The impact of BERT was immediate and sweeping: it established pre-training and fine-tuning as the dominant paradigm for NLP for several years and inspired dozens of derivative models including RoBERTa (Facebook AI, 2019), ALBERT (Google Brain, 2020), and DistilBERT (Hugging Face, 2019).

Subsequent work revealed that the NSP objective provided limited benefit (RoBERTa ablated it away entirely), but the MLM approach and the bidirectional pre-training paradigm remained highly influential. BERT-style models proved most effective on classification and extraction tasks—tasks where understanding the full input context is critical—while GPT-style models proved more effective at generation tasks. This architectural divide defined much of the landscape of NLP research and deployment through 2021 and 2022.


T5 AND THE TEXT-TO-TEXT FRAMEWORK

Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu at Google introduced T5—Text-to-Text Transfer Transformer—in a paper published in 2019. T5 reconceptualized the entire landscape of NLP tasks through a unifying framework: every task, regardless of its nature, was cast as a sequence-to-sequence text generation problem. Sentiment classification became a task where the input was the text to be classified and the output was the class label as a token (e.g., "positive" or "negative"). Translation was an input-output pair where the input included a prefix like "translate English to German:" followed by the source text. Question answering, summarization, and even coreference resolution were all reformulated in the same way. This text-to-text framework allowed a single model and a single training procedure to learn across many tasks simultaneously.

T5 was pre-trained on the Colossal Clean Crawled Corpus (C4), a cleaned version of Common Crawl comprising approximately 750 gigabytes of English text. The largest T5 model contained 11 billion parameters, establishing a new milestone at the time of its publication. T5 used a full encoder-decoder Transformer architecture and was fine-tuned on the GLUE and SuperGLUE benchmarks, among others, where it achieved state-of-the-art performance across the board. The text-to-text framework proved enormously influential, informing the design of subsequent models including FLAN (Google, 2021), which explored instruction tuning across hundreds of tasks formatted in this way, and the encoder-decoder component of several multimodal systems.


SCALING LAWS: THE PREDICTIVE SCIENCE OF MODEL TRAINING

One of the most consequential developments in LLM research was the empirical discovery that model performance scales in highly predictable ways with compute, data, and parameter count. Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei at OpenAI published "Scaling Laws for Neural Language Models" in January 2020, establishing quantitative relationships between these quantities.

The key findings of Kaplan et al. were threefold. First, model performance (measured as cross-entropy loss on held-out text) follows power laws with respect to model size (number of parameters), dataset size (number of tokens), and amount of compute used in training—over many orders of magnitude of each. Second, these power laws are approximately separable: performance depends on each factor relatively independently, so that doubling parameters while holding data and compute fixed produces roughly as much improvement as the power law predicts. Third, given a fixed computational budget, the optimal allocation of compute should primarily scale the model size, with data scaling as approximately a square root of model size—a conclusion that led OpenAI and others to train increasingly large models on fixed or modestly growing datasets.

The Kaplan et al. scaling laws profoundly shaped the research strategy of the field between 2020 and 2022, leading organizations to invest in increasingly large models—GPT-3 being the most prominent example—based on the prediction that larger models would reliably perform better. However, these laws were challenged in March 2022 by Hoffmann, Bordes, Mensch, Sifre, Moreau, Cai, Bousquet, Parmelin, and Vinyals at DeepMind, who published "Training Compute-Optimal Large Language Models," colloquially known as the Chinchilla paper.

Hoffmann et al. demonstrated that the Kaplan et al. scaling laws had systematically underestimated the importance of training data relative to model parameters. They argued that for a given compute budget, the optimal strategy is to scale parameters and tokens roughly equally: a model should be trained on approximately 20 tokens per parameter to achieve compute-optimal performance. This finding implied that many large models existing as of 2022—including GPT-3 (175B parameters but trained on only about 300 billion tokens) and Gopher (280B parameters, DeepMind, December 2021)—were significantly undertrained relative to their parameter counts.

Chinchilla itself, the model resulting from the application of these principles, contained 70 billion parameters and was trained on 1.4 trillion tokens—four times more tokens per parameter than Gopher. Despite having roughly one-quarter of Gopher's parameter count, Chinchilla matched or outperformed Gopher on the majority of downstream evaluation benchmarks, including MMLU, BIG-bench, and reading comprehension datasets. This result was highly influential, informing the design of subsequent open-source models including Meta's LLaMA series, which explicitly applied Chinchilla-style scaling to produce smaller but more capable models.


PaLM AND CHAIN-OF-THOUGHT REASONING

Google introduced PaLM—Pathways Language Model—in April 2022, described by Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, Schuh, Shi, Tsvyashchenko, Maynez, Rao, Barnes, Tay, Shazeer, Prabhakaran, Reif, Du, Hutchinson, Pope, Bradbury, Austin, Isard, Gur-Ari, Yin, Duke, Levskaya, Ghemawat, Dev, Michalewski, Garcia, Misra, Robinson, Fedus, Zhou, Ippolito, Luan, Lim, Zoph, Spiridonov, Sepassi, Dohan, Agrawal, Omernick, Dai, Pillai, Pellat, Lewkowycz, Moreira, Child, Polozov, Lee, Zhou, Wang, Saeta, Diaz, Firat, Catasta, Wei, Meier-Hellstern, Eck, Dean, Petrov, and Fiedel. The PaLM paper is notable as much for its multi-author breadth—reflecting the scale of Google's research organization—as for its technical contributions.

PaLM was trained using the Pathways system on 540 billion parameters across 6144 TPU v4 chips, making it the largest densely-activated language model at the time of publication. Its training corpus consisted of 780 billion tokens drawn from filtered webpages (a superset of C4), books, Wikipedia, news articles, GitHub code, and multilingual text, with the code component comprising approximately 5% of the total. PaLM demonstrated exceptional performance on reasoning tasks, and its publication coincided with a landmark result in chain-of-thought prompting.

Chain-of-thought (CoT) prompting, described by Wei, Wang, Schuurmans, Bosma, Ichter, Xia, Chi, Le, and Zhou in a paper published in 2022, refers to the technique of providing the model with examples that include not just the final answer but a step-by-step reasoning trace that leads to the answer. When such examples are included in the few-shot prompt, models dramatically improve on tasks requiring arithmetic, commonsense reasoning, and symbolic manipulation. Crucially, Wei et al. found that this emergent capability appeared primarily in models above approximately 100 billion parameters—smaller models showed little benefit or even degradation from chain-of-thought examples. PaLM at 540 billion parameters showed particularly striking CoT results: it achieved 58% accuracy on the GSM8K grade-school math benchmark with chain-of-thought prompting, versus previous state-of-the-art of 55% using fine-tuned models, and it solved 26% of problems in the MATH dataset of competition mathematics.

PaLM 2, released by Google in May 2023, scaled back the parameter count compared to PaLM while improving performance by training on substantially more tokens—another application of Chinchilla-style insights. PaLM 2 also incorporated much stronger multilingual capabilities and was used as the foundation for Google's Bard dialogue system and for Med-PaLM 2, which achieved expert-level performance on medical licensing examination questions.


LLaMA AND THE OPEN-SOURCE ECOSYSTEM

Meta AI released LLaMA—Large Language Model Meta AI—in February 2023, described by Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Roziere, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and Lample. LLaMA comprised a family of models in four sizes—7 billion, 13 billion, 33 billion, and 65 billion parameters—all trained using the Chinchilla-optimal approach of training smaller models on more tokens. The 7B LLaMA model was trained on 1 trillion tokens; the 65B model on 1.4 trillion tokens.

The LLaMA models were released for research use (not commercially) and their weights were made available to academic researchers. This decision had a profound and arguably underestimated effect on the field: within weeks of the weight release (which occurred through a leak in early March 2023 for the research-licensed weights), a proliferation of derivative work emerged from the open-source community. Alpaca (Stanford, March 2023) applied GPT-3.5-generated instruction data to fine-tune LLaMA-7B for instruction following; Vicuna (UC Berkeley, CMU, Stanford, and UC San Diego, March 2023) fine-tuned LLaMA-13B on user conversations shared from ShareGPT, achieving competitive performance with early versions of GPT-3.5 on certain evaluation dimensions. The LLaMA release effectively democratized access to capable open-weight language models, enabling research and deployment in resource-constrained settings.

LLaMA 2, released by Meta AI and Microsoft in July 2023, extended the model family with sizes of 7B, 13B, and 70B parameters—trained on 2 trillion tokens—and included instruction-tuned and RLHF-aligned variants called LLaMA 2-Chat. Unlike the original LLaMA models, LLaMA 2 was released under a license permitting commercial use, further accelerating adoption. The 70B LLaMA 2 model outperformed all other open-source models as of its release date on most benchmarks and was competitive with GPT-3.5 on several evaluation suites.


EMERGENT ABILITIES AND PHASE TRANSITIONS IN SCALE

One of the most striking phenomena associated with LLM scaling is the emergence of capabilities that appear abruptly at certain scale thresholds rather than improving gradually. Wei, Tay, Bommasani, Raffel, Zettlemoyer, Dasgupta, Ippolito, Veras, Anand, He, and others documented this phenomenon systematically in "Emergent Abilities of Large Language Models," published in 2022. Emergent abilities are defined as capabilities that are not present—or present only at chance levels—in smaller models but manifest in larger models above some threshold. Examples include multi-step arithmetic reasoning, word unscrambling, the ability to produce correct output given multi-step instructions, translation between low-resource language pairs, and chain-of-thought reasoning.

The emergence threshold for chain-of-thought reasoning appears at approximately 100 billion parameters, as Wei et al. 2022 documented. In-context learning—the ability to perform a new task given only a few input-output examples in the prompt without any parameter updates—similarly shows markedly improved quality in models above the 10 billion parameter range, with qualitative improvements in coherence and accuracy emerging around 60-100 billion parameters. The ability to follow complex multi-step instructions shows similar scaling behavior.

These emergence phenomena have significant implications for capability forecasting and safety. If capabilities appear abruptly rather than gradually, it is difficult to predict when a model will acquire a new ability simply by observing the trajectory of smaller models. This has motivated research into interpretability methods that can detect the presence of dangerous capabilities before they become practically exploitable, and into evaluation benchmarks specifically designed to probe for emergent abilities at the frontier of scale.

Some researchers have argued that apparent emergent abilities may be artifacts of the choice of evaluation metric rather than genuine phase transitions in model capability. A 2023 paper by Schaeffer, Miranda, and Koyejo at Stanford argued that when continuous metrics are used (rather than binary pass/fail metrics), performance on many tasks that appear to emerge abruptly actually follows smooth, predictable power-law scaling. This debate remains active, but the practical importance of large-scale evaluation and the difficulty of predicting frontier model capabilities are not in doubt.


REINFORCEMENT LEARNING FROM HUMAN FEEDBACK AND ALIGNMENT

The InstructGPT work introduced RLHF as a practical method for aligning language model behavior with human preferences. The success of that approach sparked intense research into alignment methods, the nature of human preference, and the limitations of RLHF at scale.

The RLHF pipeline, as developed at OpenAI and subsequently adopted by Google DeepMind, Anthropic, and others, consists of three stages. In the supervised fine-tuning (SFT) stage, a base language model is fine-tuned on a relatively small dataset of human demonstrations of desired behavior—typically several thousand to tens of thousands of examples. This produces a model that is better calibrated toward instruction-following but still imperfect. In the reward modeling stage, the SFT model generates multiple candidate outputs for a given prompt, and human annotators rank these outputs by quality. These preference comparisons are used to train a separate reward model (RM) using a binary cross-entropy objective: the RM learns to predict which of a pair of outputs a human annotator would prefer. Finally, in the RLHF stage, the SFT model is further fine-tuned using PPO, with the RM providing the reward signal. A key technical detail is the addition of a KL-divergence penalty between the RLHF policy and the original SFT model, which prevents the policy from optimizing the reward model in degenerate ways that do not correspond to genuine quality improvement—a failure mode known as reward hacking.

Anthropic, founded in 2021 by former OpenAI researchers including Dario Amodei, Daniela Amodei, Tom Brown, Chris Olah, and others, pursued RLHF methodology through their Claude model series. Anthropic additionally developed Constitutional AI (CAI), described in a December 2022 paper by Bai et al., which reduced reliance on human preference labeling for harmlessness by using a set of natural language principles (a "constitution") to guide model self-critique and revision, with human feedback focused on the harmfulness dimension only.


TRAINING INFRASTRUCTURE AND DATA

Training LLMs at the scales described above requires substantial infrastructure investment and careful data curation. The pre-training corpora used for the most capable models draw from multiple sources: Common Crawl, a publicly available archive of web crawl data spanning billions of web pages; Wikipedia across multiple languages; digitized books from sources including the Books3 dataset; code repositories including GitHub; scientific papers including portions of arXiv; and news corpora including news articles in multiple languages.

Data quality is a critical determinant of model quality at a given parameter and compute budget. Raw web crawl data contains vast quantities of low-quality, duplicated, and harmful content. Filtering pipelines typically apply language identification (retaining only text in the target language), quality heuristics (removing documents that are primarily lists, advertisements, or other non-prose content), deduplication (removing near-duplicate documents using MinHash or similar locality-sensitive hashing methods), and toxic content filtering. The C4 dataset (Raffel et al., 2019) pioneered a systematic approach to Common Crawl filtering that has been widely replicated and extended.

Tokenization—the process of converting raw text into discrete tokens—is another critical component of LLM training. Modern LLMs use subword tokenization algorithms, most commonly Byte-Pair Encoding (BPE), originally proposed by Gage (1994) and applied to NLP by Sennrich, Haddow, and Birch (2016). BPE starts with a character-level vocabulary and iteratively merges the most frequent adjacent pairs of symbols, building a vocabulary of subword units that achieves a balance between lexical coverage and vocabulary size. GPT-2, GPT-3, and GPT-4 all use BPE with a vocabulary of approximately 50,000 tokens (GPT-2's Byte-level BPE) or 100,000 tokens (for more recent models). SentencePiece (Kudo and Richardson, 2018) provides an alternative implementation of BPE and Unigram Language Model tokenization that operates directly on raw Unicode text without language-specific preprocessing, making it particularly suitable for multilingual models. LLaMA, PaLM, and many multilingual models use SentencePiece.

The training objective for decoder-only LLMs is Causal Language Modeling (CLM): given a sequence of tokens, predict the next token at every position in an autoregressive manner. This objective is maximally simple and computationally efficient, requiring no explicit supervision beyond raw text. For encoder-only models like BERT, Masked Language Modeling (MLM) serves as the pre-training objective. For encoder-decoder models like T5, a span corruption objective is typically used, where spans of consecutive tokens are masked and the decoder is trained to reconstruct the masked spans.

Large-scale LLM training is conducted on clusters of hundreds to thousands of accelerators—GPUs (primarily NVIDIA A100 and H100) or TPUs (Google's Tensor Processing Units)—interconnected by high-bandwidth fabrics. Training a model like GPT-3 was estimated to require approximately 3.14 × 10^23 floating-point operations, which at typical GPU efficiency takes thousands of GPU-hours and costs millions of dollars. The Megatron-LM framework (Shoeybi et al., 2019) developed at NVIDIA and the DeepSpeed framework (Rajbhandari et al., 2020) developed at Microsoft both contributed critical optimizations for distributed training, including tensor parallelism, pipeline parallelism, and ZeRO (Zero Redundancy Optimizer) memory optimization. These frameworks made it practically feasible to train models at the scale of GPT-3 and beyond.


ARCHITECTURAL DETAILS AND INNOVATIONS

The Transformer architecture introduced by Vaswani et al. 2017 consists of several core components that have been refined through successive LLM generations. The multi-head attention mechanism allows the model to jointly attend to information from different representation subspaces at different positions. Given queries Q, keys K, and values V, scaled dot-product attention is computed as Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) * V, where d_k is the dimensionality of the keys. Multi-head attention applies h attention functions in parallel, each with independently learned projection matrices, and concatenates their outputs.

Feed-forward sublayers within each Transformer layer apply two linear transformations with a non-linearity (typically ReLU or GeLU) between them: FFN(x) = max(0, xW_1 + b_1) * W_2 + b_2. In modern large models, the feed-forward layer width is typically four times the hidden dimension, though variants including the SwiGLU activation function (Shazeer, 2020) used in PaLM and LLaMA have shown improved efficiency per parameter.

Positional encoding is essential for the Transformer because the self-attention mechanism is position-invariant by design. The original Vaswani et al. 2017 paper used sinusoidal positional encodings: fixed encodings based on sine and cosine functions of different frequencies added to the token embeddings. However, sinusoidal encodings generalize poorly to sequences longer than those seen during training. Later models have adopted learned positional embeddings, Rotary Position Embedding (RoPE, Su et al., 2021) used in LLaMA and many subsequent models, and Attention with Linear Biases (ALiBi, Press et al., 2022), which adds a distance-based bias to attention logits and shows strong length generalization. RoPE encodes position by rotating the query and key vectors in complex space, enabling relative position information to be captured through inner products in a way that extrapolates better to longer sequences.

Layer normalization (Ba et al., 2016) is applied within each Transformer block to stabilize training. Modern LLMs universally use Pre-LN (applying layer normalization before the attention and feed-forward sublayers rather than after), which was found by Liu et al. (2020) to produce more stable training dynamics at large scale compared to Post-LN.

The key-value (KV) cache is a critical inference optimization: during autoregressive generation, the attention keys and values for previously generated tokens do not change and can be cached to avoid recomputation. This allows generation to scale linearly with sequence length rather than quadratically. Grouped-query attention (GQA, Ainslie et al., 2023) and multi-query attention (MQA, Shazeer, 2019) reduce the memory footprint of the KV cache by sharing key-value heads across multiple query heads, enabling significantly longer context windows and faster inference at production scale.


EVALUATION BENCHMARKS

Rigorous evaluation is essential to measure progress and compare LLMs across capabilities. The field has developed a suite of standardized benchmarks that have become central to model comparisons.

MMLU—Massive Multitask Language Understanding—was introduced by Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt at UC Berkeley in a paper published in 2021. MMLU consists of 14,042 multiple-choice questions spanning 57 subjects from elementary mathematics to professional law and medicine, drawn from actual practice examinations. Performance is measured as accuracy across all questions. GPT-4's score of 86.4% on MMLU in March 2023 substantially exceeded human expert performance on several individual subject areas.

HumanEval, introduced by Chen, Tworek, Jun, Yuan, de Oliveira Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, Ray, Puri, Krueger, Petrov, Khlaaf, Sastry, Mishkin, Chan, Gray, Ryder, Pavlov, Power, Kaiser, Bavarian, Winter, Tillet, Such, Cummings, Plappert, Chantzis, Barnes, Herbert-Voss, Guss, Nichol, Paino, Tezak, Tang, Babuschkin, Balaji, Jain, Clark, Shankar, Salmon, Brundage, Barak, Sidor, Sutskever, and Hilton in a paper published in 2021, consists of 164 Python programming problems with unit tests. Performance is measured by pass@k: the probability that at least one of k generated samples passes all unit tests. GPT-4 achieved 67% on pass@1 (first attempt success rate); GPT-3.5 achieved approximately 48%. HumanEval has become the standard benchmark for code generation capability.

GSM8K—Grade School Math 8K—was introduced by Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman at OpenAI in a paper published in 2021. GSM8K consists of 8,500 diverse grade-school mathematics word problems requiring two to eight steps of arithmetic reasoning to solve. Problems are linguistically varied and require reading comprehension as well as arithmetic. Models are evaluated on their final numerical answer; chain-of-thought prompting dramatically improves performance on this benchmark, with GPT-4 achieving 92% accuracy versus approximately 57% for GPT-3.5 without CoT. GSM8K has become the primary benchmark for evaluating step-by-step mathematical reasoning.

BIG-bench—Beyond the Imitation Game Benchmark—was introduced as a collaborative benchmark by 450 researchers across more than 130 institutions, first described in 2022 by Srivastava et al. BIG-bench contains 214 tasks spanning areas including linguistics, mathematics, common sense reasoning, biology, physics, social bias, and software development. Many tasks were specifically designed to test capabilities believed to be present in large models but not well covered by existing benchmarks. BIG-bench-Hard (BBH) is a curated subset of 23 particularly challenging tasks where model performance was closest to human performance on the original BIG-bench, making it useful for evaluating frontier models where the full BIG-bench set shows ceiling effects.

HellaSwag (Zellers, Holtzman, Bisk, Farhadi, and Choi, 2019) tests commonsense natural language inference—the ability to identify the most plausible continuation of an everyday situation described in text. TruthfulQA (Lin, Hilton, and Evans, 2022) specifically evaluates whether models produce truthful rather than plausible-sounding but incorrect answers, particularly on questions where humans exhibit systematic biases or misconceptions. ARC (Clark et al., 2018) provides grade-school science multiple-choice questions partitioned into an Easy and a Challenge set.


APPLICATIONS, LIMITATIONS, AND FUTURE DIRECTIONS

LLMs have demonstrated practical value across a broad range of applications. Code generation systems such as GitHub Copilot (launched June 2021), based on OpenAI Codex—itself a descendant of GPT-3 fine-tuned on code—demonstrated that LLMs could function as productive pair programmers, generating entire function bodies from natural language docstrings and completing partial code snippets. By 2023, GitHub reported that Copilot was responsible for approximately 46% of accepted code in repositories where it was deployed. Document summarization, translation, question answering over organizational knowledge bases, and automated customer service are additional deployment contexts where LLMs showed commercial viability.

Despite their impressive capabilities, LLMs exhibit well-documented limitations. Hallucination—the generation of factually incorrect but plausibly phrased information—remains a fundamental challenge. Because LLMs are trained to produce fluent text that resembles their training distribution rather than to retrieve and verify factual claims, they can generate confident-sounding statements that are simply incorrect. Retrieval-augmented generation (RAG, Lewis et al., 2020) addresses this by providing the model with retrieved documents as context, grounding its outputs in verifiable sources. However, RAG introduces its own challenges around retrieval quality, context length, and faithfulness.

Context length limitations constrain applications requiring processing of long documents. GPT-3 was limited to 2048 tokens; GPT-3.5-Turbo expanded this to 4096 tokens; GPT-4 was released with 8192-token and 32768-token context variants; and by mid-2023, models including Claude (Anthropic) and GPT-4 Turbo were supporting 100,000-token and 128,000-token contexts, respectively. Architectural innovations including sparse attention, sliding window attention (Longformer, Beltagy et al., 2020), and memory-augmented transformers have extended effective context windows, though the ability to reason over long documents without losing coherence remains an active research problem.

Bias, toxicity, and misuse represent persistent societal challenges associated with LLMs trained on web-scale text that reflects and amplifies human prejudices. Evaluation frameworks specifically targeting these dimensions—including WinoBias (Zhao et al., 2018), StereoSet (Nadeem et al., 2021), and BBQ (Parrish et al., 2022)—have quantified the extent of demographic and occupational biases in language models. Mitigation approaches include data filtering, counterfactual data augmentation, and RLHF-based fine-tuning specifically targeting harmful outputs.

The efficiency frontier has become increasingly important as deployment scale grows. Knowledge distillation, quantization (reducing parameter precision from float32 to float16, int8, or int4), pruning, and speculative decoding are all active areas of research aimed at reducing inference cost. Mixture-of-experts (MoE) architectures, in which only a subset of model parameters are activated for any given input, offer a path to dramatically increasing total parameter count while keeping per-inference compute constant. The Switch Transformer (Fedus, Zoph, and Shazeer, 2022) demonstrated that MoE models could achieve superior performance at equal training compute, though with increased implementation complexity and memory requirements.

The timeline from GPT-1's 117 million parameters in 2018 to GPT-3's 175 billion parameters in 2020—a factor of roughly 1,500 in 26 months—illustrates the extraordinary pace at which the field has scaled. The subsequent period from 2021 to 2023 saw not only continued scaling but qualitative shifts in the nature of capability development: the emergence of instruction following through RLHF, the democratization of capable open-weight models through the LLaMA release, the development of chain-of-thought reasoning as a reliable prompting technique, and the first successful integration of language models into products serving hundreds of millions of users. Understanding, characterizing, and improving LLMs across their full range of capabilities and failure modes remains one of the central challenges of AI research.


CONCLUSION

The development of large language models represents one of the most rapid and consequential capability expansions in the history of artificial intelligence. From the Transformer architecture introduced by Vaswani et al. in 2017, through the scaling insights of Kaplan et al. in 2020 and Hoffmann et al. in 2022, through the alignment breakthroughs of InstructGPT in 2022 and the open-source democratization enabled by LLaMA in February 2023, the field has followed a trajectory that has repeatedly confounded expectations about the pace and nature of progress. The models described in this survey—GPT-1 through GPT-4, BERT, T5, PaLM, Chinchilla, LLaMA—are not merely incremental improvements on their predecessors but qualitatively different systems that have expanded the scope of what automated natural language processing can achieve.

The scaling laws established by Kaplan et al. and refined by Hoffmann et al. provide a partial framework for predicting future progress, but they cannot account for architectural innovations, alignment breakthroughs, or the potential phase transitions associated with emergent abilities at extreme scale. What the survey of this field makes clear is that progress depends on advances across all components simultaneously: better data curation and tokenization, more efficient architectures, better alignment methodologies, improved evaluation benchmarks that can distinguish genuine capability from surface pattern matching, and deployment infrastructure that can make capable models accessible while managing their risks. The integration of multimodal inputs, agent-based planning, and tool use represent promising near-term directions. The fundamental questions of how to make LLMs reliable, interpretable, and genuinely aligned with human values remain open problems that will define the research agenda for years to come.
